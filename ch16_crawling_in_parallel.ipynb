{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ch16_crawling_in_parallel.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lblogan14/web_scraping_with_python/blob/master/ch16_crawling_in_parallel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_5knwmCRnic-",
        "colab_type": "text"
      },
      "source": [
        "#Processes versus Thread\n",
        "In computer science, each process running on an operating system can have multiple\n",
        "threads. Each process has its own allocated memory, which means that multiple\n",
        "threads can access that same memory, while multiple processes cannot and must\n",
        "communicate information explicitly."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pnj-p_xtnkxo",
        "colab_type": "text"
      },
      "source": [
        "#Multithreaded Crawling\n",
        "Use the `_thread` module"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3CKuO-nUpMMb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import _thread\n",
        "import time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ss0coCDWpOlX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def print_time(threadName, delay, iterations):\n",
        "  start = int(time.time())\n",
        "  for i in range(0, iterations):\n",
        "    time.sleep(delay)\n",
        "    seconds_elapsed = str(int(time.time()) - start)\n",
        "    print('{} {}'.format(seconds_elapsed, threadName))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U0OUu-pZpf7c",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        },
        "outputId": "5b26f865-6b9b-42d6-8c3a-2482239b9b1c"
      },
      "source": [
        "try:\n",
        "  _thread.start_new_thread(print_time, ('Fizz', 3, 33))\n",
        "  _thread.start_new_thread(print_time, ('Buzz', 5, 20))\n",
        "  _thread.start_new_thread(print_time, ('Counter', 1, 100))\n",
        "except:\n",
        "  print('Error: unable to start thread')\n",
        "  \n",
        "while 1:\n",
        "  pass"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1 Counter\n",
            "2 Counter\n",
            "3 Fizz\n",
            "3 Counter\n",
            "4 Counter\n",
            "5 Buzz\n",
            "5 Counter\n",
            "6 Fizz\n",
            "6 Counter\n",
            "7 Counter\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-3d99c8cac119>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m   \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pFAs9uPDp1jr",
        "colab_type": "text"
      },
      "source": [
        "This is a reference to the classic FizzBuzz programming test with a somewhat more verbose output."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mPN4swmjp_u5",
        "colab_type": "text"
      },
      "source": [
        "To perform a more useful task in the threads, such as crawling a website:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hKcRaL_MqEOq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from urllib.request import urlopen\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import random\n",
        "\n",
        "import _thread\n",
        "import time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UgdlidngqY1b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "visited = []\n",
        "\n",
        "def getLinks(thread_name, bsObj):\n",
        "  print('Getting links in {}'.format(thread_name))\n",
        "  links = bsObj.find('div', {'id':'bodyContent'}).find_all('a', href=re.compile('^(/wiki/)((?!:).)*$'))\n",
        "  return [link for link in links if link not in visited]\n",
        "\n",
        "def scrape_article(thread_name, path):\n",
        "  visited.append(path)\n",
        "  html = urlopen('http://en.wikipedia.org{}'.format(path))\n",
        "  time.sleep(5)\n",
        "  bsObj = BeautifulSoup(html, 'html.parser')\n",
        "  title = bsObj.find('h1').get_text()\n",
        "  print('Scraping {} in thread {}'.format(title, thread_name))\n",
        "  links = getLinks(thread_name, bsObj)\n",
        "  if len(links) > 0:\n",
        "    newArticle = links[random.randint(0, len(links)-1)].attrs['href']\n",
        "    print(newArticle)\n",
        "    scrape_article(thread_name, newArticle)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yeCjJrBUcUUf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 637
        },
        "outputId": "84589b50-e4b3-47f0-896c-f80ec5ea8157"
      },
      "source": [
        "try:\n",
        "  _thread.start_new_thread(scrape_article, ('Thread 1', '/wiki/Kevin_Bacon',))\n",
        "  _thread.start_new_thread(scrape_article, ('Thread 2', '/wiki/Monty_Python',))\n",
        "except:\n",
        "  print('Error: unable to start threads')\n",
        "  \n",
        "while 1:\n",
        "  pass"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Scraping Kevin Bacon in thread Thread 1\n",
            "Getting links in Thread 1\n",
            "/wiki/Stacy_Keach\n",
            "Scraping Monty Python in thread Thread 2\n",
            "Getting links in Thread 2\n",
            "/wiki/Razzies\n",
            "Scraping Stacy Keach in thread Thread 1\n",
            "Getting links in Thread 1\n",
            "/wiki/Planes_(film)\n",
            "Scraping Golden Raspberry Awards in thread Thread 2\n",
            "Getting links in Thread 2\n",
            "/wiki/25th_Golden_Raspberry_Awards\n",
            "Scraping 25th Golden Raspberry Awards in thread Thread 2\n",
            "Getting links in Thread 2\n",
            "/wiki/Golden_Raspberry_Awards\n",
            "Scraping Planes (film) in thread Thread 1\n",
            "Getting links in Thread 1\n",
            "/wiki/Finding_Nemo\n",
            "Scraping Golden Raspberry Awards in thread Thread 2\n",
            "Getting links in Thread 2\n",
            "/wiki/Dean_Devlin\n",
            "Scraping Finding Nemo in thread Thread 1\n",
            "Getting links in Thread 1\n",
            "/wiki/Gore_Verbinski\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-28276f341a64>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m   \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yb9EKQqMc5Oi",
        "colab_type": "text"
      },
      "source": [
        "Because now the crawling speed on Wikipedia is almost twice as fast as with a single thread, use `time.sleep(5)` to prevent the scrpt from putting too much of a load on Wikipedia's servers. In practice, when running against a server where the number of requests is not an issue, this line should be removed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nqjcinJEdPBa",
        "colab_type": "text"
      },
      "source": [
        "To keep track of the articles the threads have collectively seen, a list `visited` is used in this multi-threaded environment to store the visited websites."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NQAoFNf7nnJt",
        "colab_type": "text"
      },
      "source": [
        "##Race Conditions and Queues\n",
        "Lists are great for appending to or reading from, but not so great for removing items\n",
        "at arbitrary points, especially from the beginning of the list. Try to pass messages to threads using nonlist variables.\n",
        "\n",
        "Queues are list-like objects that operate on either a First In First Out (FIFO)\n",
        "approach or a Last In First Out (LIFO) approach. A queue receives messages from\n",
        "any thread via `queue.put('My message')` and can transmit the message to any\n",
        "thread that calls `queue.get()`.\n",
        "\n",
        "Queues are not designed to store static data, but to transmit it in a thread-safe way.\n",
        "\n",
        "Users can have a smaller number of database threads, each with its own connection, sitting around taking items from a queue and storing them. This provides a much more manageable set of database connections."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DVFvvi3OiYjP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from urllib.request import urlopen\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import random\n",
        "import _thread\n",
        "from queue import Queue\n",
        "import time\n",
        "import pymysql"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1fsSWigmin_b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def storage(queue):\n",
        "  conn = pymysql.connect(host='127.0.0.1', unix_socket='/tmp/mysql.sock', user='root', passwd='', db='mysql', charset='utf8')\n",
        "  cur = conn.cursor()\n",
        "  cur.execute('USE wiki_threads')\n",
        "  while 1:\n",
        "    if not queue.empty():\n",
        "      article = queue.get()\n",
        "      cur.execute('SELECT * FROM pages WHERE path = %s', (article[\"path\"]))\n",
        "      if cur.rowcount == 0:\n",
        "        print(\"Storing article {}\".format(article[\"title\"]))\n",
        "        cur.execute('INSERT INTO pages (title, path) VALUES (%s, %s)', (article[\"title\"], article[\"path\"]))\n",
        "        conn.commit()\n",
        "      else:\n",
        "        print(\"Article already exists: {}\".format(article['title']))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1IUmn-FBiyQL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "visited = []\n",
        "def getLinks(thread_name, bsObj):\n",
        "  print('Getting links in {}'.format(thread_name))\n",
        "  links = bsObj.find('div', {'id':'bodyContent'}).find_all('a', href=re.compile('^(/wiki/)((?!:).)*$'))\n",
        "  return [link for link in links if link not in visited]\n",
        "\n",
        "def scrape_article(thread_name, path, queue):\n",
        "  visited.append(path)\n",
        "  html = urlopen('http://en.wikipedia.org{}'.format(path))\n",
        "  time.sleep(5)\n",
        "  bsObj = BeautifulSoup(html, 'html.parser')\n",
        "  title = bsObj.find('h1').get_text()\n",
        "  print('Added {} for storage in thread {}'.format(title, thread_name))\n",
        "  queue.put({\"title\":title, \"path\":path})\n",
        "  links = getLinks(thread_name, bsObj)\n",
        "  if len(links) > 0:\n",
        "    newArticle = links[random.randint(0, len(links)-1)].attrs['href']\n",
        "    scrape_article(thread_name, newArticle, queue)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aUL4zVv-i5rn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "queue = Queue()\n",
        "try:\n",
        "  _thread.start_new_thread(scrape_article, ('Thread 1', '/wiki/Kevin_Bacon', queue,))\n",
        "  _thread.start_new_thread(scrape_article, ('Thread 2', '/wiki/Monty_Python', queue,))\n",
        "  _thread.start_new_thread(storage, (queue,))\n",
        "except:\n",
        "  print ('Error: unable to start threads')\n",
        "\n",
        "while 1:\n",
        "  pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4s9QwNwxjCzm",
        "colab_type": "text"
      },
      "source": [
        "This script creates three threads: two to scrape pages from Wikipedia in a random\n",
        "walk, and a third to store the collected data in a MySQL database."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MwziDtJqnpOe",
        "colab_type": "text"
      },
      "source": [
        "##The `threading` Module\n",
        "The Python `_thread` module allows users to micro-manage threads but does not provide a lot of higher-level functions that make life easier. The `threading` module allows users to use threads cleanly while still exposing all of the features of the underlying `_thread`.\n",
        "\n",
        "The example is shown below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0KIdz8M2lrrT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import threading\n",
        "import time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qjf2Om_3ltL8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "027d1d4b-2064-474f-b0ed-2f1122eec34f"
      },
      "source": [
        "def print_time(threadName, delay, iterations):\n",
        "  start = int(time.time())\n",
        "  for i in range(0, iterations):\n",
        "    time.sleep(delay)\n",
        "    seconds_elapsed = str(int(time.time()) - start)\n",
        "    print('{} {}'.format(seconds_elapsed, threadName))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Scraping Old Lake Highlands, Dallas in thread Thread 1\n",
            "Getting links in Thread 1\n",
            "/wiki/Kessler,_Dallas\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bd3zKdCol-cq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "threading.Thread(target=print_time, args=('Fizz', 3, 33)).start()\n",
        "threading.Thread(target=print_time, args=('Buzz', 5, 20)).start()\n",
        "threading.Thread(target=print_time, args=('Counter', 1, 100)).start()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wyj4DUo4mFDi",
        "colab_type": "text"
      },
      "source": [
        "This produces the same \"FizzBuzz\" output as the previous simple `_thread` example."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PnE7-jC1mL_x",
        "colab_type": "text"
      },
      "source": [
        "The `threading` module can also create local thread data that is unavailable to the other threads. This local data can be created at any point within the thread function by calling `threading_local()`. This solves the problem of race conditions happening between shared objects in threads."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UCi-nc65nELR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import threading"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5XWxdaJ9nFzR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def crawler(url):\n",
        "  data = threading.local()\n",
        "  data.visited = []\n",
        "  #Crawl site\n",
        "  \n",
        "threading.Thread(target=crawler, args=('http://brookings.edu')).start()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oQR_92_9nDax",
        "colab_type": "text"
      },
      "source": [
        "The `isAlive` function looks to see if the thread is still active. It will be true until a thread completes crawling (or crashes). Crawlers are designed to run for a very long time. The `isAlive` method can ensure that, if a thread crashes, it restarts."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4IYaUhhmnSAV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import threading\n",
        "import time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5XKBJM_NngYf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Crawler(threading.Thread):\n",
        "  \n",
        "  def __init__(self):\n",
        "    threading.Thread.__init__(self)\n",
        "    self.done = False\n",
        "    \n",
        "  def isDone(self):\n",
        "    return self.done\n",
        "  \n",
        "  def run(self):\n",
        "    time.sleep(5)\n",
        "    self.done = True\n",
        "    raise Exception('Something bad happened!')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MyfOM4LYnwnb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "t = Crawler()\n",
        "t.start()\n",
        "\n",
        "while True:\n",
        "  time.sleep(1)\n",
        "  if t.isDone():\n",
        "    print('Done')\n",
        "    break\n",
        "    \n",
        "  if not t.isAlive():\n",
        "    t = Crawler()\n",
        "    t.start()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bVrmTYArn-DC",
        "colab_type": "text"
      },
      "source": [
        "The `Crawler` class contains an isDone method that can be used to check if the\n",
        "crawler is done crawling.\n",
        "\n",
        "Any exceptions raised by `Crawler.run` will cause the class to be restarted until `isDone`\n",
        "is `True` and the program exits."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MJKbV3fYns4Q",
        "colab_type": "text"
      },
      "source": [
        "#Multiprocess Crawling\n",
        "The Python `Process` module creates new process objects that can be started and joined from the main process. The following example uses the FizzBuzz example:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yFd2_JF4oT7Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from multiprocessing import Process\n",
        "import time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7AgZDg2dol8w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def print_time(threadName, delay, iterations):\n",
        "  start = int(time.time())\n",
        "  for i in range(0, iterations):\n",
        "    time.sleep(delay)\n",
        "    seconds_elapsed = str(int(time.time()) - start)\n",
        "    print(threadName if threadName else seconds_elapsed)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hjskUSZ8o01g",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "3bfcc6d6-dd0c-409a-c147-901969632d4d"
      },
      "source": [
        "processes = []\n",
        "processes.append(Process(target=print_time, args=(None, 1, 100)))\n",
        "processes.append(Process(target=print_time, args=(\"Fizz\", 3, 33)))\n",
        "processes.append(Process(target=print_time, args=(\"Buzz\", 5, 20)))\n",
        "\n",
        "for p in processes:\n",
        "  p.start()\n",
        "  \n",
        "for p in processes:\n",
        "  p.join()\n",
        "  \n",
        "print('Program complete')"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Scraping Jack Hindon Medal in thread Thread 1\n",
            "Getting links in Thread 1\n",
            "/wiki/Nkwe_Medal\n",
            "1\n",
            "2\n",
            "3\n",
            "Fizz\n",
            "4\n",
            "5\n",
            "Buzz\n",
            "Scraping Cardinal number in thread Thread 2\n",
            "Getting links in Thread 2\n",
            "/wiki/Dual_number\n",
            "6\n",
            "Fizz\n",
            "Scraping Nkwe Medal in thread Thread 1\n",
            "Getting links in Thread 1\n",
            "/wiki/South_African_military_decorations_order_of_wear#Order_of_wear\n",
            "7\n",
            "8\n",
            "9\n",
            "Fizz\n",
            "10\n",
            "Buzz\n",
            "Scraping Dual number in thread Thread 2\n",
            "Getting links in Thread 2\n",
            "/wiki/Nilpotent\n",
            "11\n",
            "Scraping South African military decorations order of wear in thread Thread 1\n",
            "Getting links in Thread 1\n",
            "/wiki/Honoris_Crux_Diamond\n",
            "12\n",
            "Fizz\n",
            "13\n",
            "14\n",
            "15\n",
            "Fizz\n",
            "Buzz\n",
            "Scraping Nilpotent in thread Thread 2\n",
            "Getting links in Thread 2\n",
            "/wiki/Digital_object_identifier\n",
            "16\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Process Process-2:\n",
            "Process Process-1:\n",
            "Process Process-3:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"<ipython-input-13-aeccb3614d11>\", line 4, in print_time\n",
            "    time.sleep(delay)\n",
            "KeyboardInterrupt\n",
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"<ipython-input-13-aeccb3614d11>\", line 4, in print_time\n",
            "    time.sleep(delay)\n",
            "  File \"<ipython-input-13-aeccb3614d11>\", line 4, in print_time\n",
            "    time.sleep(delay)\n",
            "KeyboardInterrupt\n",
            "KeyboardInterrupt\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Scraping Honoris Crux Diamond in thread Thread 1\n",
            "Getting links in Thread 1\n",
            "/wiki/South_African_Defence_Force\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-0f325aca3730>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprocesses\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m   \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Program complete'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/multiprocessing/process.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_pid\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetpid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'can only join a child process'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_popen\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'can only join a started process'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_popen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m             \u001b[0m_children\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiscard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/multiprocessing/popen_fork.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m     48\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0;31m# This shouldn't block if wait() returned successfully.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWNOHANG\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0.0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/multiprocessing/popen_fork.py\u001b[0m in \u001b[0;36mpoll\u001b[0;34m(self, flag)\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                     \u001b[0mpid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwaitpid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m                     \u001b[0;31m# Child process not yet created. See #1731717\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n9OiF0T3pRfe",
        "colab_type": "text"
      },
      "source": [
        "Each process is treated as an individual independent program by the OS. The PIDs can be found using the `os` module:\n",
        "\n",
        "```\n",
        "import os\n",
        "...\n",
        "#prints the child PID\n",
        "os.getpid()\n",
        "#prints the parent PID\n",
        "os.getppid()\n",
        "```\n",
        "\n",
        "Each process in your program should print a different PID for the line `os.getpid()`,\n",
        "but will print the same parent PID on `os.getppid()`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jcrRJBgmp2-D",
        "colab_type": "text"
      },
      "source": [
        "The `join` statement here is needed to execute any code after the child processes complete."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rMTpqRYinx-4",
        "colab_type": "text"
      },
      "source": [
        "##Multiprocess Crawling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cjk_iLBKqJKe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from urllib.request import urlopen\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import random\n",
        "\n",
        "from multiprocessing import Process, Queue\n",
        "import os\n",
        "import time\n",
        "from threading import Thread"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W2grEHcaqPqp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def getLinks(bsObj, queue):\n",
        "  print('Getting links in {}'.format(os.getpid()))\n",
        "  links = bsObj.find('div', {'id':'bodyContent'}).find_all('a', href=re.compile('^(/wiki/)((?!:).)*$'))\n",
        "  return [link for link in links if link not in queue.get()]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AqT0_9KHqaNX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def scrape_article(path, queue):\n",
        "  queue.get().append()\n",
        "  print('Process {} list is now: {}'.format(os.getpid(), visited))\n",
        "  html = urlopen('http://en.wikipedia.org{}'.format(path))\n",
        "  time.sleep(5)\n",
        "  bsObj = BeautifulSoup(html, 'html.parser')\n",
        "  title = bsObj.find('h1').get_text()\n",
        "  print('Scraping {} in process {}'.format(title, os.getpid()))\n",
        "  links = getLinks(bsObj)\n",
        "  if len(links) > 0:\n",
        "    newArticle = links[random.randint(0, len(links)-1)].attrs['href']\n",
        "    print(newArticle)\n",
        "    scrape_article(newArticle)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aVpVGOO0q88m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "processes = []\n",
        "queue = Queue()\n",
        "processes.append(Process(target=scrape_article, args=('/wiki/Kevin_Bacon', queue,)))\n",
        "processes.append(Process(target=scrape_article, args=('/wiki/Monty_Python', queue,)))\n",
        "\n",
        "for p in processes:\n",
        "  p.start()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "emLJ8os8rPps",
        "colab_type": "text"
      },
      "source": [
        "Here the process is artificially slowed by including a `time.sleep(5)` so that this can be used for example purposes without placing an unreasonably high load on Wikipedia’s servers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OqW145zoreqG",
        "colab_type": "text"
      },
      "source": [
        "Crawling in separate processes is, in theory, slightly faster than crawling in separate\n",
        "threads for two major reasons:\n",
        "* Processes are not subject to locking by the GIL and can execute the same lines of\n",
        "code and modify the same (really, separate instantiations of the same) object at\n",
        "the same time.\n",
        "* Processes can run on multiple CPU cores, which may provide speed advantages if\n",
        "each of your processes or threads is processor intensive."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lkp2nJavnznz",
        "colab_type": "text"
      },
      "source": [
        "##Communicating between Processes\n",
        "Processes operate in their own independent memory, which can cause problems if\n",
        "they are expected to share information."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YwVWS5HesO4u",
        "colab_type": "text"
      },
      "source": [
        "If the static list of web pages was replaced with some sort of a scraping delegator, the scrapers could pop off a task from one queue in the form of a path to scrape (for example, */wiki/Monty_Python*) and in return, add a list of \"found URLs\" back onto a separate queue that would be processed by the scraping delegator so that only new URLs were added to the first task queue:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_O0PpXVLslI0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from urllib.request import urlopen\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import random\n",
        "from multiprocessing import Process, Queue\n",
        "import os\n",
        "import time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_PM5COuEspui",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def task_delegator(taskQueue, foundUrlsQueue):\n",
        "  #Initialize with a task for each process\n",
        "  visited = ['/wiki/Kevin_Bacon', '/wiki/Monty_Python']\n",
        "  taskQueue.put('/wiki/Kevin_Bacon')\n",
        "  taskQueue.put('/wiki/Monty_Python')\n",
        "\n",
        "  while 1:\n",
        "    #Check to see if there are new links in the foundUrlsQueue for processing\n",
        "    if not foundUrlsQueue.empty():\n",
        "      links = [link for link in foundUrlsQueue.get() if link not in visited]\n",
        "      for link in links:\n",
        "        #Add new link to the taskQueue\n",
        "        taskQueue.put(link)\n",
        "        #Add new link to the visited list\n",
        "        visited.append(link)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4D7J4RRZs0ol",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_links(bsObj):\n",
        "  links = bsObj.find('div', {'id':'bodyContent'}).find_all('a', href=re.compile('^(/wiki/)((?!:).)*$'))\n",
        "  return [link.attrs['href'] for link in links]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3paNju5As5FV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def scrape_article(taskQueue, foundUrlsQueue):\n",
        "  while 1:\n",
        "    while taskQueue.empty():\n",
        "      #Sleep 100 ms while waiting for the task queue \n",
        "      #This should be rare\n",
        "      time.sleep(.1)\n",
        "    path = taskQueue.get()\n",
        "    html = urlopen('http://en.wikipedia.org{}'.format(path))\n",
        "    time.sleep(5)\n",
        "    bsObj = BeautifulSoup(html, 'html.parser')\n",
        "    title = bsObj.find('h1').get_text()\n",
        "    print('Scraping {} in process {}'.format(title, os.getpid()))\n",
        "    links = get_links(bsObj)\n",
        "    #Send these to the delegator for processing\n",
        "    foundUrlsQueue.put(links)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q68ZfkRDs-9v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "processes = []\n",
        "taskQueue = Queue()\n",
        "foundUrlsQueue = Queue()\n",
        "processes.append(Process(target=task_delegator, args=(taskQueue, foundUrlsQueue,)))\n",
        "processes.append(Process(target=scrape_article, args=(taskQueue, foundUrlsQueue,)))\n",
        "processes.append(Process(target=scrape_article, args=(taskQueue, foundUrlsQueue,)))\n",
        "\n",
        "for p in processes:\n",
        "  p.start()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ev6Sk8nytIW5",
        "colab_type": "text"
      },
      "source": [
        "Rather than each process or thread following its own random walk from the starting\n",
        "point they were assigned, they work together to do a complete coverage crawl of the\n",
        "website. Each process can pull any “task” from the queue, not just links that they have\n",
        "found themselves."
      ]
    }
  ]
}