{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ch6_storing_data.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lblogan14/web_scraping_with_python/blob/master/ch6_storing_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q2-pzBiqqFtd",
        "colab_type": "code",
        "outputId": "0ecb932e-f87d-424c-aed1-bb05e3b0c23c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive') #add drive content to the notebook"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RJiwewRxl-So",
        "colab_type": "text"
      },
      "source": [
        "#Media Files\n",
        "The `urllib` library, used to retrieve the content of webpages also contains functions to retrieve the content of files. The following program uses `urllib.request.urlretrieve` to download images from a remote URL:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bOXFGL3ioU9w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from urllib.request import urlretrieve, urlopen\n",
        "from bs4 import BeautifulSoup"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4bx1eP54oapF",
        "colab_type": "code",
        "outputId": "13da3284-ffd1-4302-8fe4-446b3e90d6d2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "html = urlopen('http://www.pythonscraping.com')\n",
        "bs = BeautifulSoup(html, 'html.parser')\n",
        "imageLocation = bs.find('a', {'id': 'logo'}).find('img')['src']\n",
        "urlretrieve(imageLocation, 'logo.jpg')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('logo.jpg', <http.client.HTTPMessage at 0x7f4c49572518>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7dKayS8Vo05V",
        "colab_type": "text"
      },
      "source": [
        "This downloads the logo from *http://pythonscraping.com* and stores it as *logo.jpg* in\n",
        "the same directory from which the script is running."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TEf_KFOTpm9M",
        "colab_type": "text"
      },
      "source": [
        "The following downloads all internal files, linked to by any tag's `src` attribute, from the home page of *http://pythonscraping.com*."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zUY92mKPpx00",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "from urllib.request import urlopen, urlretrieve\n",
        "from bs4 import BeautifulSoup"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TIJhLG3oqEB1",
        "colab_type": "code",
        "outputId": "78ecdb48-cd16-4ccb-c410-a2335526a4f8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%cd /content/drive/My\\ \\Drive/Colab\\ \\Notebooks/Web_Scraping_with_Python"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Colab Notebooks/Web_Scraping_with_Python\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F7yL1VXTqckZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "downloadDirectory = 'download'\n",
        "baseURL = 'http://pythonscraping.com'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O00sDxxBqkCE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def getAbsoluteURL(baseURL, source):\n",
        "  if source.startswith('http://www.'):\n",
        "    url = 'http://{}'.format(source[11:])\n",
        "  elif source.startswith('http://'):\n",
        "    url = source\n",
        "  elif source.startswith('www.'):\n",
        "    url = source[4:]\n",
        "    url = 'http://{}'.format(source)\n",
        "  else:\n",
        "    url = '{}/{}'.format(baseURL, source)\n",
        "  \n",
        "  if baseURL not in url:\n",
        "    return None\n",
        "  return url"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RF0WOjORrC35",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def getDownloadPath(baseURL, absoluteURL, downloadDirectory):\n",
        "  path = absoluteURL.replace('www.','')\n",
        "  path = path.replace(baseURL, '')\n",
        "  path = downloadDirectory+path\n",
        "  directory = os.path.dirname(path)\n",
        "  \n",
        "  if not os.path.exists(directory):\n",
        "    os.makedirs(directory)\n",
        "    \n",
        "  return path"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sOCiwFMKrYNw",
        "colab_type": "code",
        "outputId": "8edfbce9-1bd6-4105-da6b-58e1ba1e80b2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        }
      },
      "source": [
        "html = urlopen('http://www.pythonscraping.com')\n",
        "bs = BeautifulSoup(html, 'html.parser')\n",
        "downloadList = bs.find_all(src=True)\n",
        "\n",
        "for download in downloadList:\n",
        "  fileURL = getAbsoluteURL(baseURL, download['src'])\n",
        "  if fileURL is not None:\n",
        "    print(fileURL)\n",
        "    \n",
        "urlretrieve(fileURL, getDownloadPath(baseURL, fileURL, downloadDirectory))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "http://pythonscraping.com/misc/jquery.js?v=1.4.4\n",
            "http://pythonscraping.com/misc/jquery.once.js?v=1.2\n",
            "http://pythonscraping.com/misc/drupal.js?pa2nir\n",
            "http://pythonscraping.com/sites/all/themes/skeletontheme/js/jquery.mobilemenu.js?pa2nir\n",
            "http://pythonscraping.com/sites/all/modules/google_analytics/googleanalytics.js?pa2nir\n",
            "http://pythonscraping.com/sites/default/files/lrg_0.jpg\n",
            "http://pythonscraping.com/img/lrg%20(1).jpg\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('download/img/lrg%20(1).jpg', <http.client.HTTPMessage at 0x7f4c490ae320>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bAlReA-FsW5T",
        "colab_type": "text"
      },
      "source": [
        "This script uses a lambda function (introduced in Chapter 2) to select all tags on the\n",
        "front page that have the `src` attribute, and then cleans and normalizes the URLs to get\n",
        "an absolute path for each download (making sure to discard external links). Then,\n",
        "each file is downloaded to its own path in the local folder downloaded."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B2OMUCiUstxO",
        "colab_type": "text"
      },
      "source": [
        "#Storing Data to CSV\n",
        "Modifying a CSV file, or creating one entirely from scratch is easy with Python's `csv` library:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ysuLulWDwDDv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import csv\n",
        "\n",
        "csvFile = open('test.csv', 'w+')\n",
        "try:\n",
        "  writer = csv.writer(csvFile)\n",
        "  writer.writerow(('number', 'number plus 2', 'number times 2'))\n",
        "  for i in range(10):\n",
        "    writer.writerow((i, i+2, i*2))\n",
        "finally:\n",
        "  csvFile.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eyE4SvwQwj2K",
        "colab_type": "text"
      },
      "source": [
        "Use BeautifulSoup and the `get_text()` function to retrieve an HTML table and write it as a CSV file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EEUgmjsowtu-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import csv\n",
        "from urllib.request import urlopen\n",
        "from bs4 import BeautifulSoup"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KafcOUmOw1fr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "html = urlopen('http://en.wikipedia.org/wiki/Comparison_of_text_editors')\n",
        "bs = BeautifulSoup(html, 'html.parser')\n",
        "\n",
        "# The main comparison table is currently the first table on the page\n",
        "table = bs.find_all('table', {'class': 'wikitable'})[0]\n",
        "rows = table.find_all('tr')\n",
        "\n",
        "csvFile = open('editors.csv', 'w+')\n",
        "writer = csv.writer(csvFile)\n",
        "try:\n",
        "  for row in rows:\n",
        "    csvRow = []\n",
        "    for cell in row.find_all(['td', 'th']):\n",
        "      csvRow.append(cell.get_text())\n",
        "    writer.writerow(csvRow)\n",
        "finally:\n",
        "  csvFile.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jalDqGBpxqsl",
        "colab_type": "text"
      },
      "source": [
        "#MySQL\n",
        "MySQL is the most popular open source relational database management system."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YTWGKI-xyfCt",
        "colab_type": "text"
      },
      "source": [
        "##Installing MySQL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "snLzHeMKyOqf",
        "colab_type": "code",
        "outputId": "5ac94520-e25c-452c-a732-93e573071a85",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!apt-get install mysql-server"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-410\n",
            "Use 'apt autoremove' to remove it.\n",
            "The following additional packages will be installed:\n",
            "  libcgi-fast-perl libcgi-pm-perl libencode-locale-perl libevent-core-2.1-6\n",
            "  libfcgi-perl libhtml-parser-perl libhtml-tagset-perl libhtml-template-perl\n",
            "  libhttp-date-perl libhttp-message-perl libio-html-perl\n",
            "  liblwp-mediatypes-perl libtimedate-perl liburi-perl mysql-client-5.7\n",
            "  mysql-client-core-5.7 mysql-server-5.7 mysql-server-core-5.7 psmisc\n",
            "Suggested packages:\n",
            "  libdata-dump-perl libipc-sharedcache-perl libwww-perl mailx tinyca\n",
            "The following NEW packages will be installed:\n",
            "  libcgi-fast-perl libcgi-pm-perl libencode-locale-perl libevent-core-2.1-6\n",
            "  libfcgi-perl libhtml-parser-perl libhtml-tagset-perl libhtml-template-perl\n",
            "  libhttp-date-perl libhttp-message-perl libio-html-perl\n",
            "  liblwp-mediatypes-perl libtimedate-perl liburi-perl mysql-client-5.7\n",
            "  mysql-client-core-5.7 mysql-server mysql-server-5.7 mysql-server-core-5.7\n",
            "  psmisc\n",
            "0 upgraded, 20 newly installed, 0 to remove and 4 not upgraded.\n",
            "Need to get 21.1 MB of archives.\n",
            "After this operation, 162 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 mysql-client-core-5.7 amd64 5.7.27-0ubuntu0.18.04.1 [7,040 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 mysql-client-5.7 amd64 5.7.27-0ubuntu0.18.04.1 [2,302 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 mysql-server-core-5.7 amd64 5.7.27-0ubuntu0.18.04.1 [7,779 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 psmisc amd64 23.1-1ubuntu0.1 [52.5 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu bionic/main amd64 libevent-core-2.1-6 amd64 2.1.8-stable-4build1 [85.9 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 mysql-server-5.7 amd64 5.7.27-0ubuntu0.18.04.1 [3,196 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu bionic/main amd64 libhtml-tagset-perl all 3.20-3 [12.1 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu bionic/main amd64 liburi-perl all 1.73-1 [77.2 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu bionic/main amd64 libhtml-parser-perl amd64 3.72-3build1 [85.9 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu bionic/main amd64 libcgi-pm-perl all 4.38-1 [185 kB]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu bionic/main amd64 libfcgi-perl amd64 0.78-2build1 [32.8 kB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu bionic/main amd64 libcgi-fast-perl all 1:2.13-1 [9,940 B]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu bionic/main amd64 libencode-locale-perl all 1.05-1 [12.3 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu bionic/main amd64 libhtml-template-perl all 2.97-1 [59.0 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu bionic/main amd64 libtimedate-perl all 2.3000-2 [37.5 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu bionic/main amd64 libhttp-date-perl all 6.02-1 [10.4 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu bionic/main amd64 libio-html-perl all 1.001-1 [14.9 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu bionic/main amd64 liblwp-mediatypes-perl all 6.02-1 [21.7 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu bionic/main amd64 libhttp-message-perl all 6.14-1 [72.1 kB]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 mysql-server all 5.7.27-0ubuntu0.18.04.1 [9,948 B]\n",
            "Fetched 21.1 MB in 2s (12.7 MB/s)\n",
            "Preconfiguring packages ...\n",
            "Selecting previously unselected package mysql-client-core-5.7.\n",
            "(Reading database ... 131289 files and directories currently installed.)\n",
            "Preparing to unpack .../00-mysql-client-core-5.7_5.7.27-0ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking mysql-client-core-5.7 (5.7.27-0ubuntu0.18.04.1) ...\n",
            "Selecting previously unselected package mysql-client-5.7.\n",
            "Preparing to unpack .../01-mysql-client-5.7_5.7.27-0ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking mysql-client-5.7 (5.7.27-0ubuntu0.18.04.1) ...\n",
            "Selecting previously unselected package mysql-server-core-5.7.\n",
            "Preparing to unpack .../02-mysql-server-core-5.7_5.7.27-0ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking mysql-server-core-5.7 (5.7.27-0ubuntu0.18.04.1) ...\n",
            "Selecting previously unselected package psmisc.\n",
            "Preparing to unpack .../03-psmisc_23.1-1ubuntu0.1_amd64.deb ...\n",
            "Unpacking psmisc (23.1-1ubuntu0.1) ...\n",
            "Selecting previously unselected package libevent-core-2.1-6:amd64.\n",
            "Preparing to unpack .../04-libevent-core-2.1-6_2.1.8-stable-4build1_amd64.deb ...\n",
            "Unpacking libevent-core-2.1-6:amd64 (2.1.8-stable-4build1) ...\n",
            "Selecting previously unselected package mysql-server-5.7.\n",
            "Preparing to unpack .../05-mysql-server-5.7_5.7.27-0ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking mysql-server-5.7 (5.7.27-0ubuntu0.18.04.1) ...\n",
            "Selecting previously unselected package libhtml-tagset-perl.\n",
            "Preparing to unpack .../06-libhtml-tagset-perl_3.20-3_all.deb ...\n",
            "Unpacking libhtml-tagset-perl (3.20-3) ...\n",
            "Selecting previously unselected package liburi-perl.\n",
            "Preparing to unpack .../07-liburi-perl_1.73-1_all.deb ...\n",
            "Unpacking liburi-perl (1.73-1) ...\n",
            "Selecting previously unselected package libhtml-parser-perl.\n",
            "Preparing to unpack .../08-libhtml-parser-perl_3.72-3build1_amd64.deb ...\n",
            "Unpacking libhtml-parser-perl (3.72-3build1) ...\n",
            "Selecting previously unselected package libcgi-pm-perl.\n",
            "Preparing to unpack .../09-libcgi-pm-perl_4.38-1_all.deb ...\n",
            "Unpacking libcgi-pm-perl (4.38-1) ...\n",
            "Selecting previously unselected package libfcgi-perl.\n",
            "Preparing to unpack .../10-libfcgi-perl_0.78-2build1_amd64.deb ...\n",
            "Unpacking libfcgi-perl (0.78-2build1) ...\n",
            "Selecting previously unselected package libcgi-fast-perl.\n",
            "Preparing to unpack .../11-libcgi-fast-perl_1%3a2.13-1_all.deb ...\n",
            "Unpacking libcgi-fast-perl (1:2.13-1) ...\n",
            "Selecting previously unselected package libencode-locale-perl.\n",
            "Preparing to unpack .../12-libencode-locale-perl_1.05-1_all.deb ...\n",
            "Unpacking libencode-locale-perl (1.05-1) ...\n",
            "Selecting previously unselected package libhtml-template-perl.\n",
            "Preparing to unpack .../13-libhtml-template-perl_2.97-1_all.deb ...\n",
            "Unpacking libhtml-template-perl (2.97-1) ...\n",
            "Selecting previously unselected package libtimedate-perl.\n",
            "Preparing to unpack .../14-libtimedate-perl_2.3000-2_all.deb ...\n",
            "Unpacking libtimedate-perl (2.3000-2) ...\n",
            "Selecting previously unselected package libhttp-date-perl.\n",
            "Preparing to unpack .../15-libhttp-date-perl_6.02-1_all.deb ...\n",
            "Unpacking libhttp-date-perl (6.02-1) ...\n",
            "Selecting previously unselected package libio-html-perl.\n",
            "Preparing to unpack .../16-libio-html-perl_1.001-1_all.deb ...\n",
            "Unpacking libio-html-perl (1.001-1) ...\n",
            "Selecting previously unselected package liblwp-mediatypes-perl.\n",
            "Preparing to unpack .../17-liblwp-mediatypes-perl_6.02-1_all.deb ...\n",
            "Unpacking liblwp-mediatypes-perl (6.02-1) ...\n",
            "Selecting previously unselected package libhttp-message-perl.\n",
            "Preparing to unpack .../18-libhttp-message-perl_6.14-1_all.deb ...\n",
            "Unpacking libhttp-message-perl (6.14-1) ...\n",
            "Selecting previously unselected package mysql-server.\n",
            "Preparing to unpack .../19-mysql-server_5.7.27-0ubuntu0.18.04.1_all.deb ...\n",
            "Unpacking mysql-server (5.7.27-0ubuntu0.18.04.1) ...\n",
            "Setting up libhtml-tagset-perl (3.20-3) ...\n",
            "Setting up libevent-core-2.1-6:amd64 (2.1.8-stable-4build1) ...\n",
            "Setting up psmisc (23.1-1ubuntu0.1) ...\n",
            "Setting up libencode-locale-perl (1.05-1) ...\n",
            "Setting up mysql-server-core-5.7 (5.7.27-0ubuntu0.18.04.1) ...\n",
            "Setting up libtimedate-perl (2.3000-2) ...\n",
            "Setting up libio-html-perl (1.001-1) ...\n",
            "Setting up liblwp-mediatypes-perl (6.02-1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1) ...\n",
            "Setting up liburi-perl (1.73-1) ...\n",
            "Processing triggers for systemd (237-3ubuntu10.24) ...\n",
            "Setting up libhtml-parser-perl (3.72-3build1) ...\n",
            "Setting up libcgi-pm-perl (4.38-1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Setting up mysql-client-core-5.7 (5.7.27-0ubuntu0.18.04.1) ...\n",
            "Setting up libfcgi-perl (0.78-2build1) ...\n",
            "Setting up libhttp-date-perl (6.02-1) ...\n",
            "Setting up libhtml-template-perl (2.97-1) ...\n",
            "Setting up libcgi-fast-perl (1:2.13-1) ...\n",
            "Setting up libhttp-message-perl (6.14-1) ...\n",
            "Setting up mysql-client-5.7 (5.7.27-0ubuntu0.18.04.1) ...\n",
            "Setting up mysql-server-5.7 (5.7.27-0ubuntu0.18.04.1) ...\n",
            "invoke-rc.d: could not determine current runlevel\n",
            "invoke-rc.d: policy-rc.d denied execution of stop.\n",
            "update-alternatives: using /etc/mysql/mysql.cnf to provide /etc/mysql/my.cnf (my.cnf) in auto mode\n",
            "Renaming removed key_buffer and myisam-recover options (if present)\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/mysql.service → /lib/systemd/system/mysql.service.\n",
            "invoke-rc.d: could not determine current runlevel\n",
            "invoke-rc.d: policy-rc.d denied execution of start.\n",
            "Setting up mysql-server (5.7.27-0ubuntu0.18.04.1) ...\n",
            "Processing triggers for systemd (237-3ubuntu10.24) ...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gd04V3q7yrfD",
        "colab_type": "text"
      },
      "source": [
        "##Basic Commands\n",
        "Except for variable names, MySQL is case *insensitive*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VcK5Yfvtz7uw",
        "colab_type": "text"
      },
      "source": [
        "To create a database:\n",
        "\n",
        "\n",
        "```\n",
        "CREATE DATABASE scraping;\n",
        "```\n",
        "\n",
        "Since every MySQL instance can have multiple databases, specify to MySQL which database is used:\n",
        "\n",
        "\n",
        "```\n",
        "USE scraping;\n",
        "```\n",
        "\n",
        "Now all commands entered will be run against the `scraping` database until the MySQL connection is closed or switched to another database.\n",
        "\n",
        "To create a table to store a collection of scraped web pages, a table in MySQL  cannot exist without columns. To define columns in MySQL, the users must enter them in a comma-delimited list, within parentheses, after the `CREATE TABLE <tablename>` statement:\n",
        "\n",
        "\n",
        "```\n",
        "CREATE TABLE pages (id BIGINT(7) NOT NULL AUTO-INCREMENT, \n",
        "  title VARCHAR(200), \n",
        "  content VARCHAR(10000),\n",
        "  created TIMESTAMP DEFAULT CURRENT_TIMESTAMP, \n",
        "  PRIMARY KEY(id));\n",
        "```\n",
        "\n",
        "This means that each column definition has three parts:\n",
        "* The name (`id, title, created,` etc)\n",
        "* The variable type (`BIGINT(7), VARCHAR, TIMESTAMP`)\n",
        "* Additional attributes (NOT NULL AUTO_INCREMENT)\n",
        "\n",
        "A table's *key* must be defined at the end of the list of columns. This key is used to organize the content in the table for fast lookups.\n",
        "\n",
        "To see what the structure of the table is:\n",
        "\n",
        "\n",
        "```\n",
        "DESCRIBE pages;\n",
        "```\n",
        "\n",
        "To insert test data into the *pages* table:\n",
        "\n",
        "\n",
        "```\n",
        "INSERT INTO pages (title, content) VALUES (\"Test page title\",\n",
        "  \"This is some test page content. It can be up to 10,000 characters long.\");\n",
        "```\n",
        "\n",
        "To override the default values:\n",
        "\n",
        "\n",
        "```\n",
        "INSERT INTO pages (id, title, content, created) VALUES (3, \n",
        "  \"Test page title\",\n",
        "    \"This is some test page content. It can be up to 10,000 characters long.\",\n",
        "    \"2019-08-03 19:49:21\");\n",
        "```\n",
        "\n",
        "To select the data:\n",
        "\n",
        "\n",
        "```\n",
        "SELECT * FROM pages WHERE id = 2;\n",
        "```\n",
        "\n",
        "This tells MySQL to select all from pages where `id` equals 2.\n",
        "\n",
        "The following returns all the rows where the `title` field contains \"test\":\n",
        "\n",
        "\n",
        "```\n",
        "SELECT * FROM pages WHERE title LIKE \"%test%\";\n",
        "```\n",
        "\n",
        "To select a piece of data with multiple columns:\n",
        "\n",
        "\n",
        "```\n",
        "SELECT id, title FROM pages WHERE content LIKE \"%page content%\";\n",
        "```\n",
        "\n",
        "This returns the `id` and `title` where the content contains the phrase \"page content\".\n",
        "\n",
        "To delete some data:\n",
        "\n",
        "\n",
        "```\n",
        "DELETE FROM pages WHERE id = 1;\n",
        "```\n",
        "\n",
        "To update some data:\n",
        "\n",
        "\n",
        "```\n",
        "UPDATE pages SET title = \"A new title\",\n",
        "  content = \"Some new content\" WHERE id=2;\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ftfBvx928ZhV",
        "colab_type": "text"
      },
      "source": [
        "##Integrating with Python\n",
        "Install PyMySQL first:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NHvOblox8j4T",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "cba717f8-3081-4110-a9bc-a8d46585cd75"
      },
      "source": [
        "!pip3 install PyMySQL"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting PyMySQL\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ed/39/15045ae46f2a123019aa968dfcba0396c161c20f855f11dea6796bcaae95/PyMySQL-0.9.3-py2.py3-none-any.whl (47kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 2.9MB/s \n",
            "\u001b[?25hInstalling collected packages: PyMySQL\n",
            "Successfully installed PyMySQL-0.9.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JPwV-ycy8l_n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pymysql\n",
        "conn = pymysql.connect(host = '127.0.0.1', \n",
        "                       unix_socket = '/tmp/mysql.sock',\n",
        "                       user = 'root',\n",
        "                       passwd = 'root',\n",
        "                       db = 'mysql',\n",
        "                       charset = 'utf8')\n",
        "cur = conn.cursor()\n",
        "cur.execute('USE scraping')\n",
        "cur.execute('SELECT * FROM pages WHERE id=1')\n",
        "print(cur.fetchone())\n",
        "cur.close()\n",
        "conn.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hl5l5xKO92Mm",
        "colab_type": "text"
      },
      "source": [
        "The connection object (`conn`) and the cursor object (`cur`) are commonly used in database programming. \\\\\n",
        "The connection is responsible for connecting to the database but also sending the database information, handling rollbacks (when a query or set of queries needs to be aborted, and the database needs to be returned to its previous state), and creating new cursor objects. \\\\\n",
        "A connection can have many cursors. A cursor keeps track of certain state information, such as which database it is using. A cursor also contains the results of the latest query it has executed. Calling functions on the cursor, such as `cur.fetchone()` is used to access this information. \\\\\n",
        "It is important that both the cursor and the connection are closed after they no longer used."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zqVJ1Dg5-_w6",
        "colab_type": "text"
      },
      "source": [
        "By default, MySQL does not handle Unicode. The following code is used to turn on this feature:\n",
        "\n",
        "\n",
        "```\n",
        "ALTER DATABASE scraping CHARACTER SET = utf8mb4 COLLATE = utf8mb4_unicode_ci;\n",
        "ALTER TABLE pages CONVERT TO CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci;\n",
        "ALTER TABLE pages CHANGE title title VARCHAR(200) CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci;\n",
        "ALTER TABLE pages CHANGE content content VARCHAR(10000) CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci;\n",
        "```\n",
        "\n",
        "These four lines change the default character set for the database, for the table, and for both of the two columns -- from `utf8mb4` (still technically Unicode, but with notoriously terrible support for most Unicode characters) to `utf8mb4_unicode_ci`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cNt3gZ7N_pag",
        "colab_type": "text"
      },
      "source": [
        "The following can be executed once the database is prepared to accept a wide variety of all that Wikipedia can throw at it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "USglcDCD_zqz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from urllib.request import urlopen\n",
        "from bs4 import BeautifulSoup\n",
        "import datetime\n",
        "import random\n",
        "import pymysql\n",
        "import re"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "azvbztvR_7k9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "conn = pymysql.connect(host='127.0.0.1', \n",
        "                       unix_socket='/tmp/mysql.sock',\n",
        "                       user='root', \n",
        "                       passwd='root', \n",
        "                       db='mysql', \n",
        "                       charset='utf8')\n",
        "cur = conn.cursor()\n",
        "cur.execute('USE scraping')\n",
        "\n",
        "random.seed(datetime.datetime.now())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N1amIC3dACPK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def store(title, content):\n",
        "  cur.execute('INSERT INTO pages (title, content) VALUES (\"%s\", \"%s\")', (title, content))\n",
        "  cur.connection.commit()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kg03oHL-AI0F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def getLinks(articleUrl):\n",
        "  html = urlopen('http://en.wikipedia.org'+articleUrl)\n",
        "  bs = BeautifulSoup(html, 'html.parser')\n",
        "  title = bs.find('h1').get_text()\n",
        "  content = bs.find('div', {'id':'mw-content-text'}).find('p').get_text()\n",
        "  store(title, content)\n",
        "  return bs.find('div', {'id':'bodyContent'}).findAll('a', href=re.compile('^(/wiki/)((?!:).)*$'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NAHbi2JuAMBp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "links = getLinks('/wiki/Kevin_Bacon')\n",
        "try:\n",
        "  while len(links) > 0:\n",
        "    newArticle = links[random.randint(0, len(links)-1)].attrs['href']\n",
        "    print(newArticle)\n",
        "    links = getLinks(newArticle)\n",
        "finally:\n",
        "  cur.close()\n",
        "  conn.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "094e5FgFATJJ",
        "colab_type": "text"
      },
      "source": [
        "The added `charset='utf8'` tells the connection that it should send all information to the database as UTF-8. \\\\\n",
        "In the `store` function, the cursor has stored information about the database and its own context, and then operate through the connection in order to send information back to the database and insert information. \\\\\n",
        "The `finally` statement ensures that the cursor and the connection will both be closed immediately before the program ends regardless of how the program is interrupted or the exceptions that might be thrown."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "39eFJSFRBLS0",
        "colab_type": "text"
      },
      "source": [
        "##Database Techniques and Good Practice\n",
        "1. Always add `id` columns to the tables so MySQL at least know one way to order it\n",
        "2. Use intelligent indexing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HJmFLMTkBy0C",
        "colab_type": "text"
      },
      "source": [
        "##\"Six Degrees\" in MySQL\n",
        "The following will store all pages on Wikipedia that have a “Bacon number” (the number of links between it and the page for Kevin Bacon, inclusive) of 6 or less:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sa1ZaNs_CiBz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from urllib.request import urlopen\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import pymysql\n",
        "from random import shuffle"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7TNfJI1FCkt6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "conn = pymysql.connect(host='127.0.0.1', unix_socket='/tmp/mysql.sock',\n",
        "                       user='root', passwd='root', db='mysql', charset='utf8')\n",
        "cur = conn.cursor()\n",
        "cur.execute('USE wikipedia')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J7ypjBo4Cpog",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def insertPageIfNotExists(url):\n",
        "    cur.execute('SELECT * FROM pages WHERE url = %s', (url))\n",
        "    if cur.rowcount == 0:\n",
        "        cur.execute('INSERT INTO pages (url) VALUES (%s)', (url))\n",
        "        conn.commit()\n",
        "        return cur.lastrowid\n",
        "    else:\n",
        "        return cur.fetchone()[0]\n",
        "\n",
        "def loadPages():\n",
        "    cur.execute('SELECT * FROM pages')\n",
        "    pages = [row[1] for row in cur.fetchall()]\n",
        "    return pages\n",
        "\n",
        "def insertLink(fromPageId, toPageId):\n",
        "    cur.execute('SELECT * FROM links WHERE fromPageId = %s AND toPageId = %s', \n",
        "                  (int(fromPageId), int(toPageId)))\n",
        "    if cur.rowcount == 0:\n",
        "        cur.execute('INSERT INTO links (fromPageId, toPageId) VALUES (%s, %s)', \n",
        "                    (int(fromPageId), int(toPageId)))\n",
        "        conn.commit()\n",
        "def pageHasLinks(pageId):\n",
        "    cur.execute('SELECT * FROM links WHERE fromPageId = %s', (int(pageId)))\n",
        "    rowcount = cur.rowcount\n",
        "    if rowcount == 0:\n",
        "        return False\n",
        "    return True\n",
        "\n",
        "def getLinks(pageUrl, recursionLevel, pages):\n",
        "    if recursionLevel > 4:\n",
        "        return\n",
        "\n",
        "    pageId = insertPageIfNotExists(pageUrl)\n",
        "    html = urlopen('http://en.wikipedia.org{}'.format(pageUrl))\n",
        "    bs = BeautifulSoup(html, 'html.parser')\n",
        "    links = bs.findAll('a', href=re.compile('^(/wiki/)((?!:).)*$'))\n",
        "    links = [link.attrs['href'] for link in links]\n",
        "\n",
        "    for link in links:\n",
        "        linkId = insertPageIfNotExists(link)\n",
        "        insertLink(pageId, linkId)\n",
        "        if not pageHasLinks(linkId):\n",
        "            print(\"PAGE HAS NO LINKS: {}\".format(link))\n",
        "            pages.append(link)\n",
        "            getLinks(link, recursionLevel+1, pages)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1kTvWJKACrml",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "getLinks('/wiki/Kevin_Bacon', 0, loadPages()) \n",
        "cur.close()\n",
        "conn.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wy8wasQMDcGI",
        "colab_type": "text"
      },
      "source": [
        "#Email\n",
        "The web pages are sent over HTTP, the email is sent over SMTP (Simple Mail Transfer Protocol) \\\\\n",
        "The following code is used to send an email with Python assuming that an SMTP client is run locally:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oyD4yQqGEItQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import smtplib\n",
        "from email.mine.text import MIMEText"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kaeSFZbYE19J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "msg = MIMEText('The body of the email is here')\n",
        "\n",
        "msg['Subject'] = 'An Email Alert'\n",
        "msg['From'] = 'sender@gmail.com'\n",
        "msg['To'] = 'receiver@gmail.com'\n",
        "\n",
        "s = smtplib.SMTP('localhost')\n",
        "s.send_message(msg)\n",
        "s.quit()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oqGjW2wfFQ9f",
        "colab_type": "text"
      },
      "source": [
        "The `smtplib` package contains information for handling the connection to the server.\n",
        "Just like a connection to a MySQL server, this connection must be torn down every\n",
        "time it is created, to avoid creating too many connections."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B73zxeQAFVSp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import smtplib\n",
        "from email.mime.text import MIMEText\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.request import urlopen\n",
        "import time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5lH3CEhzFYdZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sendMail(subject, body):\n",
        "    msg = MIMEText(body)\n",
        "    msg['Subject'] = subject\n",
        "    msg['From'] ='sender@gmail.com'\n",
        "    msg['To'] = 'receiver@gmail.com'\n",
        "\n",
        "    s = smtplib.SMTP('localhost')\n",
        "    s.send_message(msg)\n",
        "    s.quit()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XY0jfVkdFce4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bs = BeautifulSoup(urlopen('https://isitchristmas.com/'), 'html.parser')\n",
        "while(bs.find('a', {'id':'answer'}).attrs['title'] == 'NO'):\n",
        "    print('It is not Christmas yet.')\n",
        "    time.sleep(3600)\n",
        "    bs = BeautifulSoup(urlopen('https://isitchristmas.com/'), 'html.parser')\n",
        "\n",
        "sendMail('It\\'s Christmas!', 'According to http://itischristmas.com, it is Christmas!')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fbrTPdJXFlwG",
        "colab_type": "text"
      },
      "source": [
        "This code checks the website once a hour. If it sees anything other than a NO, it will send you an email alerting you that it's Christmas. \\\\\n",
        "This type of scraper can email you alerts in response to site outages, test faulures, or even the appearance of an out-of-stock product."
      ]
    }
  ]
}